{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is will bring you through your first deep reinforcement learning model\n",
    "\n",
    "\n",
    "* Seaquest game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "\n",
    "\n",
    "## New to Lasagne and AgentNet?\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global params.\n",
    "\n",
    "#game title. full list of games = http://yavar.naddaf.name/ale/list_of_current_games.html\n",
    "GAME=\"seaquest\"\n",
    "\n",
    "#game image will be resized from (210,160) to your image_size. \n",
    "#You may want a bigger image for your homework assignment IF you want a larger NN\n",
    "IMAGE_W,IMAGE_H = IMAGE_SIZE =(105,80)\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "N_AGENTS = 5\n",
    "SEQ_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f18d29d4510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM0AAAD/CAYAAABSDlLPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0AiRiYJGiEwUNEJkoqARIpP/DyslpQaMwGoOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18d30dd410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from env import Atari\n",
    "\n",
    "#creating a game\n",
    "atari = Atari(GAME,image_size=IMAGE_SIZE) \n",
    "\n",
    "action_names = np.array(atari.get_action_meanings())\n",
    "\n",
    "obs = atari.step(0)[0]\n",
    "\n",
    "plt.imshow(obs,interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DimshuffleLayer\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,IMAGE_W,IMAGE_H,3))\n",
    "\n",
    "#reshape to [sample, color, x, y] to allow for convolutional layers to work correctly\n",
    "observation_reshape = DimshuffleLayer(observation_layer,(0,3,1,2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Conv2DLayer,Pool2DLayer,DenseLayer,batch_norm,dropout\n",
    "\n",
    "#main neural network body\n",
    "conv0 = Conv2DLayer(observation_reshape,16,filter_size=(8,8),stride=(4,4),name='conv0')\n",
    "\n",
    "conv1 = Conv2DLayer(conv0,32,filter_size=(4,4),stride=(2,2),name='conv1')\n",
    "\n",
    "dense0 = DenseLayer(conv1,100,name='dense',nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "#please set this to your last layer for convenience\n",
    "last_layer = dense0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(last_layer,\n",
    "                   num_units = atari.action_space.n,\n",
    "                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                   name=\"q-evaluator layer\")\n",
    "\n",
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer,name=\"e-greedy action picker\")\n",
    "\n",
    "action_layer.epsilon.set_value(np.float32(0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=qvalues_layer,\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[conv0.W,\n",
       " conv0.b,\n",
       " conv1.W,\n",
       " conv1.b,\n",
       " dense.W,\n",
       " dense.b,\n",
       " q-evaluator layer.W,\n",
       " q-evaluator layer.b]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "[2016-10-18 06:19:00,319] We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "from pool import AtariGamePool\n",
    "\n",
    "pool = AtariGamePool(agent,GAME, N_AGENTS,image_size=IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE'\n",
      "  'RIGHTFIRE']\n",
      " ['RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE'\n",
      "  'RIGHTFIRE']\n",
      " ['DOWNLEFTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'LEFT' 'RIGHTFIRE' 'RIGHTFIRE'\n",
      "  'RIGHTFIRE']\n",
      " ['RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE'\n",
      "  'RIGHTFIRE']\n",
      " ['RIGHTFIRE' 'RIGHTFIRE' 'NOOP' 'RIGHTFIRE' 'RIGHTFIRE' 'RIGHTFIRE'\n",
      "  'RIGHTFIRE']]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "CPU times: user 572 ms, sys: 380 ms, total: 952 ms\n",
      "Wall time: 499 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_names[action_log])\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "_,_,_,_,qvalues_seq = agent.get_sessions(\n",
    "    pool.experience_replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    optimize_experience_replay=True,\n",
    ")\n",
    "\n",
    "actions = pool.experience_replay.actions  #replay actions\n",
    "rewards = pool.experience_replay.rewards  #replay rewards\n",
    "is_alive= pool.experience_replay.is_alive #alive indicator. alive=0 means death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#crop rewards to [-1,+1] to avoid explosion.\n",
    "import theano.tensor as T\n",
    "rewards = T.maximum(-1,T.minimum(rewards,1))\n",
    "\n",
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      actions,\n",
    "                                                      rewards,\n",
    "                                                      is_alive,\n",
    "                                                      gamma_or_gammas=0.99,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-18 08:45:57,334] Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\n",
      "[2016-10-18 08:45:57,337] Clearing 3 monitor files from previous run (because force=True was provided)\n",
      "[2016-10-18 08:45:57,441] Starting new video recorder writing to /home/jheuristic/PycharmProjects/atari_non_flicker/records/openaigym.video.3.6325.video000000.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-990b7c1fff2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muntrained_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./records\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecord_video\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/jheuristic/PycharmProjects/atari_non_flicker/pool.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, n_games, save_path, record_video, verbose)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_memories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mprev_memories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_memories\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/Documents/gym/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/Documents/gym/gym/monitoring/monitor.pyc\u001b[0m in \u001b[0;36m_after_step\u001b[1;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# Record video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/Documents/gym/gym/monitoring/video_recorder.pyc\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/Documents/gym/gym/monitoring/video_recorder.pyc\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidFrame\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tried to pass invalid video frame, marking as broken: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/Documents/gym/gym/monitoring/video_recorder.pyc\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1.9.0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_path=\"./records/openaigym.video.0.13.video000000.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1\tepsilon=0.500\n",
      "iter=11\tepsilon=0.495\n",
      "iter=21\tepsilon=0.491\n",
      "iter=31\tepsilon=0.486\n",
      "iter=41\tepsilon=0.482\n",
      "iter=51\tepsilon=0.478\n",
      "iter=61\tepsilon=0.473\n",
      "iter=71\tepsilon=0.469\n",
      "iter=81\tepsilon=0.465\n",
      "iter=91\tepsilon=0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-18 06:22:12,869] Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\n",
      "[2016-10-18 06:22:12,871] Clearing 4 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ae58457a2a9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch_counter\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0maction_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_video\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0maction_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/PycharmProjects/atari_non_flicker/pool.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, n_games, save_path, record_video, verbose)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_memories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mprev_memories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_memories\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/Documents/gym/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/PycharmProjects/atari_non_flicker/env.pyc\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# need to call this guy to then correctly remove flickering\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "for i in xrange(10**10):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH)\n",
    "    loss = train_step()\n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/1000.)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,current_epsilon))\n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "        action_layer.epsilon.set_value(np.float32(0))\n",
    "        rewards[epoch_counter] = pool.evaluate(record_video=False)\n",
    "        action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "        \n",
    "        plt.title(\"random frames\")\n",
    "        for i in range(min((len(pool.games),6))):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.imshow(pool.games[i].get_observation())\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f42015ddcd0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAL3iZptGuFVQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f42021cc5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*zip(*sorted(list(rewards.items()),key=lambda p:p[0])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rw = pool.evaluate(n_games=20,save_path=\"./records\",record_video=True)\n",
    "print \"mean session score=%f.5\"%rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./records/openaigym.video.0.13.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "#select the one you want\n",
    "video_path=\"./records/openaigym.video.0.13.video000000.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. qlearning_n_step), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for info?\n",
    "* [lasagne doc](http://lasagne.readthedocs.io/en/latest/)\n",
    "* [agentnet doc](http://agentnet.readthedocs.io/en/latest/)\n",
    "* [gym homepage](http://gym.openai.com/)\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    " \n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
